## Abstract

首先此文章中的网络是一种卷积核不唯一的盲去模糊网络（这也是神经网络的特性）。传统的能量函数优化方法大多数基于一个简单的假设——卷积核是唯一的，但是很明显现实中模糊核的来源还包含很多难以预测的condition。除此之外，对于当前的很多机器学习方法来说，由于目前数据集大多数都来源于模拟的模糊（也就是说实际上模糊的线性计算得到的，而不是现实中计算得到的），所以现实中若是模糊核无法参数化或者说是近似的时候就无法取得好的结果。

总而言之：==预测卷积核的方法并不是一个很适用于现实的方法，所以深度学习方法有其必要性==

*以上的原因其实是深度学习中含有很多非线性计算，也就是激活函数，原则上能够表示非线性的非唯一的卷积核。*

同时此文章还提供了一个真实的清晰与模糊图对形成的数据集。

这篇文章的总思路实际上是通过深度学习的方法与传统的金字塔去模糊方法的结合，使算法能够接纳更多现实噪音。



数据集：

https://github.com/SeungjunNah/DeepDeblur_release



## Introduction

目前大多数卷积模型如下：
$$
\mathbf{B}=\mathbf{K S}+\mathbf{n}
$$
也就是模糊图像是模糊参数对原图像影响后加上噪声的值。由于解出这里面的模糊参数是一个不适定问题。因此在很多方法中会设计一些简单的假设来约束解space。

比如假设模糊参数全部来自于相机：

| Paper                                                  |
| ------------------------------------------------------ |
| Non-uniform deblurring for shaken images               |
| Single image deblurring using motion density functions |

比如来自相机和物体运动：

| Paper                    |
| ------------------------ |
| Dynamic scene deblurring |

比如假设模糊核是线性的并同时估计原图片和运动过程

| Paper                                      |
| ------------------------------------------ |
| Segmentation-free dynamic scene deblurring |

**但以上的假设均无法排除一些突发的移动（这样的移动是非连续和偶然的）**





除了传统方法的以外。还有使用CNN的方法。但一个根本的问题就是这些训练过程没有真实的模糊和高清图像对。都是用线性卷积核模拟模糊。所以对应的网络训练出来的结果只能适用于一些线性模糊核的实际图像。



**此文章目的是解决这一类的问题**

方法是使用多尺寸的CNN方法。跳过假设模糊核的过程。值得注意的是。在这个实现中使用了一种叫做adversarial loss的方法。相关的论文如下

| Paper                       |
| --------------------------- |
| Generative adversarial nets |



### Related Works

上文已经涉及

但在章末说明了这个多尺寸神经网络能够避免图片在经过CNN的过程中被剪切。



### Kernel-Free Learning for Dynamic Scene Deblurring

估计具体的模糊核面临如下的问题：

1. 太多无法预测的噪音。比如景深会带来聚焦的问题。
2. 模糊核的预测会很大程度上受到噪声的影响。
3. 为每个像素设计模糊核是一个需要耗费非常大计算量的事情。



所以设计kernel-free methods。

为了让整个网络脱离核。作者采用的方法是尽量真实的去还原现实中的模糊。由于高帧率的摄像头无法同时拍摄模糊和不模糊的两种图片。所以在这里作者从相机成像的流程的角度设计了一个新的通过连续帧清晰图像还原模糊图像的方法。他们称这种方法能够非常大程度的还原真实。同时在拍摄的过程中也有人为加入的一些模糊。



## Blur Dataset

这一整章讲述如何处理得到模糊数据集的。



首先模糊还原的根本难点在于相机照相过程中会把每个图片经过一个CRF函数变成另一个图片。也就是:
$$
\hat{S}(t)=g(S(t))
$$
这里的g就是CRF函数。这个函数通常是无法预测的。但这个函数有一个均值的近似。也就是
$$
g(x)=x^{1 / \gamma}\\
\gamma=2.2
$$
我们知道一张图片是在一个曝光时间内获取的图像信息的均值。**再经过CRF这个非线性函数映射到一个新的图片展现给我们的。**传统的方式是将CRF映射后的一个个图片求均值得到图像信息。

**如果CRF是一个线性的函数这种叠加是能够模拟模糊的**。

但是CRF不是线性的。所以在这个文章中作者通过CRF函数的平均的近似来取得反函数。通过反函数得到原图然后叠加取均值。最后再次映射。

这样的流程保证了最终的图片从统计的意义上是能够近似真实的模拟结果的。因为整个计算过程还原了相机曝光的过程。



## Proposed Method



### Net Design

整个网络是由残差网络发展而来。

| Paper                                        |
| -------------------------------------------- |
| Deep residual learning for image recognition |

==待学==







![image-20210812172328401](C:\Users\wanwankan\AppData\Roaming\Typora\typora-user-images\image-20210812172328401.png)

从最底层说起：

首先最底层的输入是一个1/4尺寸的图片（也就是金字塔的顶端）。经过一个卷积层变成64个通道。之后经过19个Res然后经过一个卷积层。总共40个卷积层（一个Res中含有两个卷积层）。最后的图片经过一个反卷积的过程得到下一个尺寸的输入尺寸。然后和金字塔中的下一尺寸拼接在一起。



**这里比较有趣的是作者认为采取上卷积而不是简单的上采样更加符合实际。原因是上卷积能够非线性的变换低尺寸所包含的低频信息（也就是变化缓慢的信息）。而上层本身也包含了底层所拥有的低频信息。所以两者一起能够有效的降低低频信息的冗余。而实际证明也是如此**





最终层除了最终不再需要上卷积以外其他的流程都和低层一样。



### Training



**Data Augmentation**

使用了随机剪切、旋转、颜色的变换（通过随机改变RGB图的channel来实现）、饱和度的变换（乘一个[0.5 1.5]之间的随机值）以及高斯噪声

这里引入了一个图像的描述space也就是HSV space。

![img](https://bkimg.cdn.bcebos.com/pic/b151f8198618367ab5bcd4792e738bd4b31ce559?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U3Mg==,g_7,xp_5,yp_5/format,f_auto)

其中value是一个图的明度。saturation是饱和度。hue是色调。也就是说本文中对其中的hue和saturation进行了调整（saturation代表了一个颜色中包含的色谱上颜色的比例。通常一个颜色是由一个色谱颜色再机上白色形成。saturation代表了其中色谱颜色所占的比例）

详细的Data Augmentation：

https://zhuanlan.zhihu.com/p/89419960

==待学==



**loss**

在这篇文章中使用的是组合的loss。

**Multi-scale content loss**

这个loss是综合每一层的生成清晰图和真实清晰图的区别生成的loss。其根本来源于MSE criterion（Mean Square Error）也就是均方误差。这个误差的目的是衡量每个像素的平均误差的平方。

$$
\mathcal{L}_{\text {cont }}=\frac{1}{2 K} \sum_{k=1}^{K} \frac{1}{c_{k} w_{k} h_{k}}\left\|L_{k}-S_{k}\right\|^{2}
$$
**Adversarial loss**

详情见==GAN.md==

